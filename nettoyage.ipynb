{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet d'analyse d'alimentation des citoyens de pays différents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie de nettoyage des données\n",
    "\n",
    "Dans un premier et dans le but de faciliter la compréhension des données, l'objectif est d'analyser celles-ci quantitativement et qualitativement. \n",
    "\n",
    "La partie suivante inclue la mise en évidence des valeurs manquantes avec au moins 3 méthodes de traitement adatpées aux variables concernées et d'identification et quantification de valeurs abbérantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lit le fichier `fr.openfoodfacts.org.products.csv` avec le séparateur `\\t`. Les données seront dans la variable `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fr.openfoodfacts.org.products.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut d'ores et déjà formatter les valeurs NaN\n",
    "data.replace(['None', 'nan', None, 'NaN', ' '], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse quantitative des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut déjà définir une fonction qui nous permettra plus tard d'identifier le pourcentage de Nulls pour une colonne\n",
    "def defined_percentage(data, col):\n",
    "    c = data.loc[data[col].notna(), col]\n",
    "    return (len(c) / data.shape[0]) * 100, c\n",
    "\n",
    "# On lis les valeurs non definies\n",
    "missing = data.isna()\n",
    "# On les compte et on créer un nouveau DataFrame\n",
    "missing_sum = pd.DataFrame(missing.sum(), columns=['nb_manquants'])\n",
    "# On calcul et ajoute le ratio du nombre de valeurs non définies au DataFrame\n",
    "missing_sum[\"ratio\"] = ((missing_sum[\"nb_manquants\"] / len(data)) )\n",
    "\n",
    "# On va construire un graphique pour nous montrer le nombre de valeurs manquantes\n",
    "plt.title(\"Ratio de valeurs manquantes par colonnes\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.xlabel(\"Colonnes\")\n",
    "plt.xticks([])\n",
    "plt.plot(missing_sum.sort_values([\"ratio\"])[\"ratio\"])\n",
    "plt.show()\n",
    "# On garde les ratio == 1\n",
    "missing_sum_1 = missing_sum[missing_sum[\"ratio\"] == 1]\n",
    "\n",
    "ratio_missing = missing_sum[\"nb_manquants\"].sum() / data.size\n",
    "\n",
    "print(f\"Sur tout le dataset, on a un taux de valeurs manquantes qui équivaut à {(ratio_missing*100):.2f} %.\")\n",
    "print(f\"Sur {len(missing_sum)} colonnes, il y en a {len(missing_sum_1)} qui n'ont aucune valeurs\")\n",
    "print(f\"Les colonnes qui n'ont aucune valeurs sont les suivantes :\")\n",
    "print(missing_sum_1.index.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conclusion de cette analyse est qu'actuellement nous disposons de 25 % de données exploitables. On aura certainement une valeur différente après le traitement des valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conservation des données utiles au projet uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.filter([\n",
    "    \"url\",\n",
    "    \"product_name\",\n",
    "    \"generic_name\",\n",
    "    \"quantity\", \n",
    "    \"packaging\",\n",
    "    \"packaging_tags\",\n",
    "    \"countries_tags\",\n",
    "    \"image_url\",\n",
    "    \"image_small_url\",\n",
    "    \"nutrition-score-fr_100g\",\n",
    "    \"additives_n\",\n",
    "    \"fat_100g\"\n",
    "    ], axis=1)\n",
    "\n",
    "missing_ratio = pd.DataFrame(data.isna().sum(), columns=['nb_manquants']).sum() / data.size\n",
    "\n",
    "print(f\"Après conservation des données utiles au projet, il n'y a plus que {(missing_ratio[0]*100):.2f} % de valeurs manquantes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des erreurs sur les variables générales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les champs `product_name` et `generic_name`\n",
    "\n",
    "On pourra utiliser principalement le `product_name` pour récupérer le nom du produit et en cas d'erreur essayer d'utiliser le `generic_name`.\n",
    "Ainsi on peut donc d'abord corriger les données de `generic_name` pour être sûr que la valeur qui sera possiblement remplacée dans `generic_name` sera valide. Ensuite on pourra corriger les autres données de `product_name` qui n'ont pas été remplacée mais qui malgré tout ne sont pas acceptables.\n",
    "\n",
    "Si malgré tout il y a des valeurs qui ne sont pas définies, on gardera malgré tout ces lignes car ce qui compte est d'effectuer des comparaisons avec d'autres données. De plus, si on ne conservait pas ces lignes, comme on peut le voir dans le script, il ne nous resterait peu de données à exploiter.\n",
    "\n",
    "Les traitements à effectuer sur `generic_name` sont les suivants :\n",
    "- Ne pas prendre en compte les nulls\n",
    "- Il y a des champs avec uniquement un espace à l'intérieur. On ne peut corriger ces lignes\n",
    "- Ne pas prendre en compte les noms trop long car sinon ce n'est plus un nom mais une description. On ne peut corriger ces lignes en séparant à l'avance par une virgule qui permet de dans la plupart des cas d'écrire plusieurs fois le même nom mais de manière différente.\n",
    "\n",
    "Les traitements à effectuer sur `product_name` sont les suivants:\n",
    "- Ne pas prendre en compte les nulls\n",
    "- Remplacer les valeurs non valide par sa valeur `generic_name`, `generic_name` étant valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On split sur la virgule et récupère le dernier élément qui semble être le plus représentatif d'un vrai nom.\n",
    "data.loc[\"product_name\"] = data[\"product_name\"].str.split(\",\").str[-1]\n",
    "\n",
    "# -------------\n",
    "# generic_name\n",
    "\n",
    "# On conserve les non null\n",
    "generic_exists = data[\"generic_name\"].notna()\n",
    "# On conserve les lignes qui ne contiennent pas uniquement des espaces\n",
    "generic_no_space = data[\"generic_name\"].ne(\" \")\n",
    "# On conserve les noms en dessous de 50 caractères\n",
    "generic_not_long = data[\"generic_name\"].str.len() < 50\n",
    "# Test booléen\n",
    "generic = generic_exists & generic_no_space & generic_not_long\n",
    "# Table qui contient les éléments qui respectent les critères précédents\n",
    "generic_match = data.loc[generic, [\"generic_name\"]]\n",
    "print(f\"Il y a {generic_match.size} generic_names sur {data.shape[0]} qui sont considérés comme valide, soit {((generic_match.size/data.shape[0])*100):.2f} %\")\n",
    "\n",
    "# -------------\n",
    "# product_name\n",
    "\n",
    "# Retirer les nulls\n",
    "product_exists = data[\"product_name\"].notna()\n",
    "# Table qui contient les éléments qui respectent les critères précédents\n",
    "product_match = data.loc[product_exists, [\"product_name\"]]\n",
    "\n",
    "# Colonnes qui ne contiennent pas de product_name mais contiennent un generic_name\n",
    "no_product_with_generic = (~product_exists) & generic\n",
    "print(f\"Il y a {no_product_with_generic.value_counts()[True]} lignes pour lesquelles il existe un generic_name valide mais pas de product_name valide\")\n",
    "\n",
    "# Remplacer les product_name non valide par son generic_name valide dans data\n",
    "data.loc[no_product_with_generic, [\"product_name\"]] = data.loc[no_product_with_generic, [\"generic_name\"]]\n",
    "\n",
    "# On peut désormait retirer generic_name qui n'est plus d'aucune utilité\n",
    "data.drop([\"generic_name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La colonne `countries_tags`\n",
    "\n",
    "Pour le projet il est important de recenser les pays qui sont mentionnés dans nos données. Dans les données proposée, il y avais également le champ `countries` mais j'ai remarqué qu'il y avait quelques erreurs dedans. Je préfère créer un dictionnaire qui va me permettre de convertir les tags en nom pour pouvoir les utiliser plus tard. C'est ce qui me permet de corriger de la façon la plus simple les erreurs de lexiques.\n",
    "\n",
    "Certains tags ne veulent rien dire, par exemple avec la valeur `Catégories complétées`. On peut donc éliminer ce type de lignes.\n",
    "\n",
    "Il y a également un mixe entre les pays avec des valeurs en `en:...` et `fr:...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.countriesMap as cm\n",
    "import re\n",
    "\n",
    "countries_tags_count = data[\"countries_tags\"].value_counts()\n",
    "\n",
    "# On récupère toutes les valeurs possibles de tags et on en garde 1 seul exemplaire\n",
    "all_countries = pd.DataFrame(countries_tags_count.index.str.split(',')).explode(0).drop_duplicates()\n",
    "# Il y a des pays qui ne contiennent pas le string \":\". Ce ne sont donc pas des pays\n",
    "all_countries = all_countries.loc[all_countries[0].str.contains(\":\")]\n",
    "# On retire les préfixes\n",
    "all_countries[0].replace('^.*:','', regex=True, inplace=True)\n",
    "# Cela peut entrainé des doublons, par exemple fr:france et en:france => france et france\n",
    "all_countries.drop_duplicates(inplace=True)\n",
    "\n",
    "def map_country(c_list):\n",
    "    c_list = list(map(lambda y: re.sub('^.*:','', y),c_list))\n",
    "    try :\n",
    "        # On map le nom de pays dans la langue inconnue à son pays en français\n",
    "        c_list = list(map(lambda y: cm.countries[y],c_list))\n",
    "    except:\n",
    "        return np.NaN\n",
    "    return \",\".join(c_list)\n",
    "\n",
    "# Tous les pays définis\n",
    "defined_countries = ~data[\"countries_tags\"].isna()\n",
    "# On créer une colonne \"countries\" dans laquelle on met le mapping de leur tag \"countries_tags\"\n",
    "data.loc[defined_countries, \"countries\"] = data.loc[defined_countries, \"countries_tags\"].str.split(\",\").apply(map_country)\n",
    "\n",
    "# TODO Remplacer \"Tous\" par tous les pays disponibles\n",
    "\n",
    "# TODO Remplacer \"gulf_countries\" par les pays du Golf\n",
    "# gulf-countries replace in all_countries\n",
    "gulf_countries = [\"Émirats Arabes Unis\",\"Oman\",\"Koweit\",\"Arabie Saoudite\", \"Qatar\"][True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant la colonne des pays avec le bon nom.\n",
    "\n",
    "On peut donc retirer la colonne \"countries_tags\" et retirer les valeurs null restante de \"countries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant que le traitement est effectué, on peut supprimer \"countries_tags\"\n",
    "data.drop(\"countries_tags\", axis=1, inplace=True)\n",
    "# Retirer les lignes qui n'ont pas de pays\n",
    "defined_countries = ~data[\"countries\"].isna()\n",
    "data = data.loc[defined_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut regarder les nombre de fois qu'un pays est dans une ligne\n",
    "data[\"countries\"].str.split(\",\").explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le nutriscore\n",
    "\n",
    "Notre mesure la plus importante est le nutriscore. C'est principalement grâce à elle que l'on pourra comparer les pays. Les données de base sont propres. Les conversions entre le nutriscore français (a,b,c,d,e) et la version anglais avec des nombres (entre -15 et 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde les valeurs nulls de nutrition-score-fr_100g\n",
    "missing = data[\"nutrition-score-fr_100g\"].isna() | (data[\"nutrition-score-fr_100g\"].isin([\"nan\"]))\n",
    "# On récupère le nombre de True normalisé\n",
    "n = missing.value_counts(normalize=True)[True]\n",
    "\n",
    "print(f\"Il y a {(n*100):.2f} % de valeurs de nutri-score non définies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les valeurs du nutriscore qui ne sont pas définies pourront être potentiellement calculées plus tard.\n",
    "# Pour le moment on se contente de supprimer les lignes où le nutriscore est null\n",
    "data = data[~missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recalcule le nutri score depuis les valeurs numériques\n",
    "data[\"nutri_score\"] = pd.cut(data[\"nutrition-score-fr_100g\"], bins=[-15, 0, 3, 11, 19, 40], labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données autour du nutriscore\n",
    "\n",
    "Il serait certainement utile d'utiliser des données relatives au calcul du nutriscore afin de moi même calculer celui-ci. Je pourrai ensuite, si le besoin d'analyse serait pertinent, modifier le calcul et effectuer de nouveau mon analyse.\n",
    "\n",
    "Dans ce cours, il m'est demandé de \"mettre en évidence les valeurs manquantes et d'utiliser au moins 3 méthodes de traitement adaptées aux variables concernées\". On va donc choisir une de ces methodes pour le `fat_100g` dans un premier temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, fat_100g = defined_percentage(data, \"fat_100g\")\n",
    "print(f\"Il y a {t:.2f} % de valeurs définies pour fat_100g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je peux donc utiliser une des techniques à ma disposition pour remplir les 14 % non définis restant.\n",
    "Mais en réalité on ne prends pas en compte les valeurs abbérantes. En effet, les valeurs abbérantes ne sont pas considérées comme nulls. Alors on doit également utiliser la méthode correction des valeurs abbérantes pour traiter ces cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avec valeurs abbérantes\")\n",
    "plt.boxplot(fat_100g, labels=[\"fat_100g\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Sans valeurs abbérantes\")\n",
    "fat_100g_correct = data[\"fat_100g\"].notna() & (data[\"fat_100g\"] <= 100) & (data[\"fat_100g\"] >= 0)\n",
    "fat_100g = data.loc[fat_100g_correct, \"fat_100g\"]\n",
    "plt.boxplot(fat_100g, labels=[\"fat_100g\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant qu'on a identifié les valeurs atypiques, on peut les traiter\n",
    "# On pourra utiliser la moyenne pour traiter les valeurs manquantes puisqu'elle n'est désormais plus affaiblie par les valeurs abbérantes.\n",
    "# En effet, il n'y a que des valeurs correctes. On aurait pu remplacer par la valeur médiane pour faire notre correction. (sans filtrer les valeurs abbérantes)\n",
    "# Réalisons une imputation par la moyenne :\n",
    "data.loc[~fat_100g_correct, \"fat_100g\"] = round(fat_100g.mean(), 2)\n",
    "fat_100g = data[\"fat_100g\"]\n",
    "plt.boxplot(fat_100g, labels=[\"fat_100g\"])\n",
    "plt.show()\n",
    "print(\"On peut vérifier qu'on a bien aucune valeur null\")\n",
    "t, fat_100g = defined_percentage(data, \"fat_100g\")\n",
    "print(f\"Il y a {t} % de valeurs définies pour fat_100g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareil pour le sucre, on peut utiliser cette fois-ci la médiane et supprimer ensuite les valeurs abbérantes\n",
    "print(\"Sans valeurs abbérantes\")\n",
    "fat_100g_correct = data[\"fat_100g\"].notna() & (data[\"fat_100g\"] <= 100) & (data[\"fat_100g\"] >= 0)\n",
    "fat_100g = data.loc[fat_100g_correct, \"fat_100g\"]\n",
    "plt.boxplot(fat_100g, labels=[\"fat_100g\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('oc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4682fd4cc275dd3b54e0a35cb379629832c3925d282fc495d205b56d070a9985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
